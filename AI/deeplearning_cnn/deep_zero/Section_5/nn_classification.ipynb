{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワークの構築（分類）\n",
    "出力が対応する枠に分類される確率となる問題、すなわち分類問題を扱います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ●実装するニューラルネットワーク\n",
    "今回は、以下に示す3層のニューラルネットワークを実装します。  \n",
    "\n",
    "<img src=\"images/nn_classification.png\">\n",
    "\n",
    "このニューラルネットワークは、入力層（ニューロン数: n=2）、中間層（n=2）、出力層（n=2）の3層構造です。  \n",
    "回帰の際と、層の数及び入力層、中間層は変わりませんが、出力層にはニューロンが2つある点が異なります。  \n",
    "\n",
    "出力層の活性化関数には、以前に解説したソフトマックス関数を用います。  \n",
    "このニューラルネットワークに入力を順伝播させて、出力を今回もグリッドで表示します。  \n",
    "そして、出力の傾向を観察します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ●各層の実装  \n",
    "中間層は、回帰の場合と変わりません。  \n",
    "出力層では、活性化関数として以前に解説したソフトマックス関数を用います。  \n",
    "今回も、以下のように関数を用いて出力層を実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 出力層\n",
    "def output_layer(x, w, b):\n",
    "    u = np.dot(x, w) + b\n",
    "    return np.exp(u)/np.sum(np.exp(u)) # ソフトマックス関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ソフトマックス関数では出力を確率として解釈可能なので、出力層の2つのニューロンの出力を比較して、大きい方にネットワークの入力を分類します。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ●全体のコード\n",
    "全体のコードは、以下の通りです。  \n",
    "分類結果はリストに格納して、散布図で表示します。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# x、y座標\n",
    "X = np.arange(-1.0, 1.0, 0.1)  # 要素数は20個\n",
    "Y = np.arange(-1.0, 1.0, 0.1)\n",
    "\n",
    "# 重み\n",
    "w_im = np.array([[1.0,2.0],\n",
    "                 [2.0,3.0]])  # 中間層 2x2の行列\n",
    "w_mo = np.array([[-1.0,1.0],\n",
    "                 [1.0,-1.0]]) # 出力層 2x2の行列\n",
    "\n",
    "# バイアス\n",
    "b_im = np.array([0.3,-0.3]) # 中間層\n",
    "b_mo = np.array([0.4,0.1])  # 出力層 \n",
    "\n",
    "# 中間層\n",
    "def middle_layer(x, w, b):\n",
    "    u = np.dot(x, w) + b\n",
    "    return 1/(1+np.exp(-u)) # シグモイド関数\n",
    "\n",
    "# 出力層\n",
    "def output_layer(x, w, b):\n",
    "    u = np.dot(x, w) + b\n",
    "    return np.exp(u)/np.sum(np.exp(u)) # ソフトマックス関数\n",
    "\n",
    "# 分類結果を格納するリスト\n",
    "x_1 = []\n",
    "y_1 = []\n",
    "x_2 = []\n",
    "y_2 = []\n",
    "\n",
    "# グリッドの各マスでニューラルネットワークの演算\n",
    "for i in range(20):\n",
    "    for j in range(20):\n",
    "        \n",
    "        # 順伝播\n",
    "        inp = np.array([X[i], Y[j]])\n",
    "        mid = middle_layer(inp, w_im, b_im)\n",
    "        out = output_layer(mid, w_mo, b_mo)\n",
    "        \n",
    "        # 確率の大小を比較し、分類する\n",
    "        if out[0] > out[1]:\n",
    "            x_1.append(X[i])\n",
    "            y_1.append(Y[j])\n",
    "        else:\n",
    "            x_2.append(X[i])\n",
    "            y_2.append(Y[j])\n",
    "\n",
    "# 散布図の表示\n",
    "plt.scatter(x_1, y_1, marker=\"+\")\n",
    "plt.scatter(x_2, y_2, marker=\"o\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "格子状に表される入力が、2つの領域に分類される様子が観察できます。  \n",
    "回帰の場合と比較して結果に連続性はありませんが、境界が明確になっています。  \n",
    "重みとバイアスの値を変えれば境界が様々に変化しますので、ぜひ試してみましょう。  \n",
    "\n",
    "今回は入力を2つに分類しましたが、出力層のニューロンの数を増やせば3つ以上に分類することも可能です。  \n",
    "また、ニューロンや層の数を増やすことで、さらに複雑な境界で入力を分類できるようになります。  \n",
    "\n",
    "回帰だけではなく分類のケースでも、ニューラルネットワークの表現力を確認することができました。  \n",
    "層を重ねることによる表現力の向上が、ディープラーニングの意義になります。  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
