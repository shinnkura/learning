{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 損失関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ●二乗和誤差\n",
    "出力値と正解値の差を二乗し、全ての出力層のニューロンで総和をとったものを**二乗和誤差**と呼びます。  \n",
    "二乗和誤差は、$E$を誤差、$y_k$を出力層の各出力値、$t_k$を正解値として以下の式で定義されます。\n",
    "\n",
    "$$ E = \\frac{1}{2} \\sum_{k}(y_k-t_k)^2 $$\n",
    "\n",
    "$y_k$と$t_k$の差を二乗し、全ての出力層のニューロンで総和をとり1/2をかけています。  \n",
    "後にで改めて解説しますが、1/2をかけるのは微分の際に扱いやすくするためです。\n",
    "\n",
    "二乗和誤差を用いることにより、ニューラルネットワークの出力がどの程度正解と一致しているかを定量化することができます。  \n",
    "二乗和誤差は正解や出力が連続的な数値であるケースに向いているため、回帰問題でよく使用されます。 \n",
    "\n",
    "二乗誤差は、Numpyのsum関数、square関数を用いて次のように実装することができます。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def square_sum(y, t):\n",
    "    return 1.0/2.0 * np.sum(np.square(y - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "squre_sum関数により二乗和誤差を計算します。  \n",
    "引数のyは出力で、tが正解になります。\n",
    "\n",
    "この関数をテストしてみましょう。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def square_sum(y, t):\n",
    "    return 1.0/2.0 * np.sum(np.square(y - t))\n",
    "\n",
    "y = np.array([2, 2, 2, 2])\n",
    "t = np.array([1, 1, 1, 1])\n",
    "print(square_sum(y, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力yは2が4つの配列で、正解tは1が4つの配列です。  \n",
    "これらの差の総和は4ですが、これを2で割っているので、関数の返り値は2になります。  \n",
    "square_sum関数が問題なく二乗和誤差を計算できていることが確認できました。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ●交差エントロピー誤差\n",
    "**交差エントロピー誤差**は二つの分布の間のズレを表す尺度で、分類問題でよく使用されます。  \n",
    "交差エントロピー誤差は、以下の式のように、出力$y_k$の対数と正解値の積の総和を、マイナスにしたもので表されます。\n",
    "\n",
    "（式 1）\n",
    "$$ E = - \\sum_{k}t_k \\log (y_k) $$\n",
    "\n",
    "自然対数については、以前のセクションで解説しています。  \n",
    "それでは、（式 1）の意味を解説します。  \n",
    "まず、この式を次のように変形します。\n",
    "\n",
    "（式 2）\n",
    "$$ E = \\sum_{k}t_k (-\\log (y_k)) $$\n",
    "\n",
    "分類問題における正解値は、1が1つで残りが0のone-hot表現になります。  \n",
    "従って、右辺のシグマ内で$t_k$が1の項のみ誤差に影響を与えることになり、$t_k$が0の項の影響は無視されます。  \n",
    "その結果、正解値が1のたった一つの項しか誤差に影響を与えないことになります。 \n",
    "\n",
    "$-\\log (y_k)$に関しては、グラフで考えてみましょう。$y = -\\log x$をグラフで表すと次のようになります。\n",
    "\n",
    "<img src=\"images/minus_log.png\">\n",
    "\n",
    "$-\\log x$は$x$が1の時は0で、$x$が0に近づくにつれて無限に大きくなります。  \n",
    "この$-\\log x$の性質により、$-\\log (y_k)$は正解に近づくほど小さくなり、正解から離れるにつれてどこまでも大きくなります。  \n",
    "従って、（式 2）は出力が正解から離れるほど誤差がどこまでも大きくなり、出力が正解に近づくほど誤差が0に近づくことを意味します。\n",
    "\n",
    "交差エントロピーの利点の一つは、出力値と正解値の隔離が大きい時に学習速度が速い点です。  \n",
    "上記の図からも分かる通り、出力が正解と隔離すると誤差が無限に向かって増大するように定義されているので、このような場合学習速度がとても速くなり隔離が大きく解消されます。\n",
    "\n",
    "交差エントロピー誤差は、Numpyのsum関数、及び自然対数を計算するlog関数を用いて次のように実装することができます。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy(y, t):  # 出力、正解\n",
    "    return - np.sum(t * np.log(y + 1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log関数の中身が0になると、自然対数が無限小に発散してしまい計算を続けることができなくなってしまうので、それを防ぐために`y`に微小な値`1e-7`を加えています。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
