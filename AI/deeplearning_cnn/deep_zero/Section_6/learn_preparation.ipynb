{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習の準備\n",
    "以降は、最小限の実装でバックプロパゲーションを構築します。  \n",
    "まずは、学習のためのデータとネットワークを回帰と分類それぞれで用意します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ●学習の準備 -回帰-\n",
    "ニューラルネットワークに、以下のコードで描かれるsin関数を学習させます。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-np.pi, np.pi)\n",
    "y = np.sin(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sin関数は上記のような滑らかな曲線を描きます。\n",
    "\n",
    "ニューラルネットワークの設定ですが、$x$座標をネットワークへの入力、$y$座標をネットワークからの出力とします。  \n",
    "そして、sin関数の値を正解とします。  \n",
    "出力と正解の誤差を伝播させて重みとバイアスを修正することを繰り返すと、ネットワークは少しずつsin関数を学習していきます。  \n",
    "  \n",
    "sin関数は連続的な関数なので、このケースは回帰問題になります。  \n",
    "今回は、以下のような入力層のニューロンが1つ、中間層のニューロンが3つ、出力層のニューロンが1つのシンプルなネットワークを使用します。\n",
    "\n",
    "<img src=\"images/nn_sin_regression.png\">\n",
    "\n",
    "その他設定は以下の通りにします。\n",
    "\n",
    "|||\n",
    "|:-:|:-:|\n",
    "| 中間層の活性化関数 | シグモイド関数 |\n",
    "| 出力層の活性化関数 | 恒等関数 |\n",
    "| 損失関数 | 二乗和誤差 |\n",
    "| バッチサイズ | 1 |\n",
    "|||\n",
    "\n",
    "回帰問題なので出力層の活性化関数は恒等関数とし、損失関数は二乗和誤差とします。  \n",
    "\n",
    "また、バッチサイズは1なので、オンライン学習になります。  \n",
    "コードをシンプルにするため、コード全体はオンライン学習のみ対応にします。  \n",
    "しかしながら、各層の実装はバッチ学習やミニバッチ学習にも対応できるようにします。\n",
    "\n",
    "今回は全てのデータを訓練データとし、テストデータは用意しません。  \n",
    "これは、今回は学習の様子を観察するに留めるためです。  \n",
    "\n",
    "入力と正解を用意するために、以下のコードを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- 入力と正解の用意 --\n",
    "input_data = np.linspace(-np.pi, np.pi)  # 入力\n",
    "correct_data = np.sin(input_data)  # 正解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力データははlinspace関数で用意します。  \n",
    "$-\\pi$から$\\pi$までの範囲で入力データを用意した後に、NumPyのsin関数で正解データを作ります。  \n",
    "\n",
    "今回はテストデータは使用しませんが、テストデータを作成する場合は入力データ、正解データを訓練用とテスト用に分割します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ●学習の準備 -分類-\n",
    "ニューラルネットワークに入力したx、y座標がsinカーブよりも上の領域に位置するか、下の領域に位置するかを学習させます。  \n",
    "以下の図に示すように、出力層における2つのニューロンの出力が、上の領域、下の領域、それぞれの領域に属する確率を表すようにします。 \n",
    "\n",
    "<img src=\"images/sin_classification.png\">\n",
    "\n",
    "正解は[0, 1]もしくは[1, 0]のone-hot表現とします。出力と正解の誤差を伝播させて重みとバイアスを修正することを繰り返すと、ネットワークは少しずつ正確に分類できるようになっていきます。  \n",
    "今回は、以下に示す入力層のニューロンが2つ、中間層のニューロンが6つ、出力層のニューロンが2つのシンプルなネットワークを使用します。\n",
    "\n",
    "<img src=\"images/nn_sin_classification.png\">\n",
    "\n",
    "その他設定は以下の通りにします。\n",
    "\n",
    "|||\n",
    "|:-:|:-:|\n",
    "| 中間層の活性化関数 | シグモイド関数 |\n",
    "| 出力層の活性化関数 | ソフトマックス関数 |\n",
    "| 損失関数 | 交差エントロピー誤差 |\n",
    "| バッチサイズ | 1 |\n",
    "|||\n",
    "\n",
    "入力と正解を用意するために、以下のコードを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- 座標 --\n",
    "X = np.arange(-1.0, 1.1, 0.1)\n",
    "Y = np.arange(-1.0, 1.1, 0.1)\n",
    "\n",
    "# -- 入力、正解データを作成 --\n",
    "input_data = []\n",
    "correct_data = []\n",
    "for x in X:\n",
    "    for y in Y:\n",
    "        input_data.append([x, y])\n",
    "        if y < np.sin(np.pi * x):  # y座標がsinカーブよりも下であれば\n",
    "            correct_data.append([0, 1])  # 下の領域\n",
    "        else:\n",
    "            correct_data.append([1, 0])  # 上の領域\n",
    "            \n",
    "n_data = len(correct_data)  # データ数\n",
    "\n",
    "input_data = np.array(input_data)\n",
    "correct_data = np.array(correct_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X座標、Y座標をNumPyのarrange関数により用意します。   \n",
    "そして、入力、正解データをfor文による2重のループで作成します。  \n",
    "y座標がsinカーブよりも下であるかどうかで、異なる正解を`correct_data`に入れています。  \n",
    "以上により、各座標がsinカーブの上下のどちらに属するかを表す、正解のデータを作ることができます。\n",
    "今回も全てのデータを訓練データとし、テストデータは作成しません。\n",
    "\n",
    "以上により、バックプロパゲーションによる学習のコードを書く準備ができました。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
