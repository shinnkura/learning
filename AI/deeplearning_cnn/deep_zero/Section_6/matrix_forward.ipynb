{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 行列による順伝播の演算\n",
    "バッチ学習やミニバッチ学習に対応するため、これ以降は層への入力、層からの出力、及び正解を行列で表現します。  \n",
    "今回は、行列を用いた順伝播の演算について解説します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ●行列の形式\n",
    "以下の図に、バッチ学習やミニバッチ学習に対応した行列の形式を示します。  \n",
    "それぞれ層への入力、層からの出力、正解を表す3つの行列が示されています。  \n",
    "\n",
    "<img src=\"images/matrix_batch.png\">\n",
    "\n",
    "これらの行列は行数がバッチサイズに等しく、列数はそれぞれ、層への入力数、層からの出力数、正解数となっています。  \n",
    "層からの出力数は、層のニューロン数と等しくなります。  \n",
    "これらの行列の各行は、各サンプルに対応します。    \n",
    "\n",
    "例えばバッチサイズが8、入力数（上の層のニューロン数）が3であれば、層への入力を表す行列のサイズは8x3になります。  \n",
    "バッチサイズが1、層のニューロン数が3であれば、層からの出力を表す行列のサイズは1x3となり、ベクトルのような形状になります。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ●行列による順伝播\n",
    "順伝播においては、行列積により入力と重みの積の総和を得ることができます。  \n",
    "ニューロン数が$n$の層における順伝播を考えましょう。  \n",
    "バッチサイズを$h$、入力の数（上の層のニューロン数）を$m$とすれば、入力を表す行列のサイズは、h x mになります。  \n",
    "また、重みを表す行列のサイズをm x nとします。  \n",
    "\n",
    "以下の式では、入力と重みの行列積を求めています。  \n",
    "$X$が入力を表す行列で、$W$が重みを表す行列です。  \n",
    "\n",
    "$$  \\begin{aligned} \\\\\n",
    "XW & = \\left(\n",
    "    \\begin{array}{cc}\n",
    "      x_{11} & x_{12} & \\ldots & x_{1m} \\\\\n",
    "      x_{21} & x_{22} & \\ldots & x_{2m} \\\\\n",
    "      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "      x_{h1} & x_{h2} & \\ldots & x_{hm} \\\\\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "\\left(\n",
    "    \\begin{array}{cccc}\n",
    "      w_{11} & w_{12} & \\ldots & w_{1n} \\\\\n",
    "      w_{21} & w_{22} & \\ldots & w_{2n} \\\\\n",
    "      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "      w_{m1} & w_{m2} & \\ldots & w_{mn} \\\\\n",
    "    \\end{array}\n",
    "  \\right) \\\\\n",
    "  & = \\left(\n",
    "    \\begin{array}{cccc}\n",
    "      \\sum\\limits_{k=1}^m x_{1k}w_{k1} & \\sum\\limits_{k=1}^m x_{1k}w_{k2} & \\ldots & \\sum\\limits_{k=1}^m x_{1k}w_{kn} \\\\\n",
    "       \\sum\\limits_{k=1}^m x_{2k}w_{k1} & \\sum\\limits_{k=1}^m x_{2k}w_{k2} & \\ldots & \\sum\\limits_{k=1}^m x_{2k}w_{kn} \\\\\n",
    "      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "      \\sum\\limits_{k=1}^m x_{hk}w_{k1} & \\sum\\limits_{k=1}^m x_{hk}w_{k2} & \\ldots & \\sum\\limits_{k=1}^m x_{hk}w_{kn} \\\\\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "  \\end{aligned}\n",
    "  $$ \n",
    "\n",
    "各要素が、入力と重みの積の総和である行列を得ることができました。  \n",
    "この行列は、行数がバッチサイズで、列数がこの層のニューロン数になります。 \n",
    "\n",
    "この行列にバイアスを足し合わせますが、バイアスはベクトルで形状が合わないので、Numpyのブロードキャストという機能を使用します。  \n",
    "ブロードキャストを使うと、各行に対してベクトルを足すことができます。  \n",
    "バイアスは次のように表されます。\n",
    "\n",
    "$$ \\vec{b} = (b_1, b_2, \\cdots, b_n) $$\n",
    "\n",
    "このバイアスをブロードキャストにより足し合わせた行列は、以下のようになります。\n",
    "\n",
    "$$  \\begin{aligned} \\\\\n",
    "U & = \\left(\n",
    "    \\begin{array}{cccc}\n",
    "      \\sum\\limits_{k=1}^m x_{1k}w_{k1}+b_{1} & \\sum\\limits_{k=1}^m x_{1k}w_{k2}+b_{2} & \\ldots & \\sum\\limits_{k=1}^m x_{1k}w_{kn}+b_{n} \\\\\n",
    "       \\sum\\limits_{k=1}^m x_{2k}w_{k1}+b_{1} & \\sum\\limits_{k=1}^m x_{2k}w_{k2}+b_{2} & \\ldots & \\sum\\limits_{k=1}^m x_{2k}w_{kn}+b_{n} \\\\\n",
    "      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "      \\sum\\limits_{k=1}^m x_{hk}w_{k1}+b_{1} & \\sum\\limits_{k=1}^m x_{hk}w_{k2}+b_{2} & \\ldots & \\sum\\limits_{k=1}^m x_{hk}w_{kn}+b_{n} \\\\\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "  \\end{aligned}\n",
    "  $$ \n",
    "\n",
    "この行列の各要素を活性化関数$f$で処理し、出力$Y$を得ます。  \n",
    "\n",
    "$$  \\begin{aligned} \\\\\n",
    "Y & = f(U) \\\\\n",
    " & = \\left(\n",
    "    \\begin{array}{cccc}\n",
    "      f(\\sum\\limits_{k=1}^m x_{1k}w_{k1}+b_{1}) & f(\\sum\\limits_{k=1}^m x_{1k}w_{k2}+b_{2}) & \\ldots & f(\\sum\\limits_{k=1}^m x_{1k}w_{kn}+b_{n}) \\\\\n",
    "       f(\\sum\\limits_{k=1}^m x_{2k}w_{k1}+b_{1}) & f(\\sum\\limits_{k=1}^m x_{2k}w_{k2}+b_{2}) & \\ldots & f(\\sum\\limits_{k=1}^m x_{2k}w_{kn}+b_{n}) \\\\\n",
    "      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "      f(\\sum\\limits_{k=1}^m x_{hk}w_{k1}+b_{1}) & f(\\sum\\limits_{k=1}^m x_{hk}w_{k2}+b_{2}) & \\ldots & f(\\sum\\limits_{k=1}^m x_{hk}w_{kn}+b_{n}) \\\\\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "  \\end{aligned}\n",
    "  $$\n",
    "\n",
    "層の出力$Y$は、h x n、すなわち(バッチサイズ)x(ニューロン数)の行列になります。  \n",
    "\n",
    "上記の式は、コードにするととても短い表記で済みます。  \n",
    "以下はNumpyのdot関数を用いて表記した例です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u = np.dot(x, w) + b\n",
    "y = 1/(1+np.exp(-u))  # シグモイド関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数にはシグモイド関数を使用しています。  \n",
    "`x`は入力の行列で、`w`は重みの行列、`b`がバイアスのベクトルです。  \n",
    "`u`の各要素を活性化関数で処理して、この層の出力`y`を得ます。  \n",
    "\n",
    "以上により、順伝播を行列の演算で表現することができました。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
