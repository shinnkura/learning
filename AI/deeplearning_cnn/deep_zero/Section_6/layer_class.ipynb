{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 層のクラスによる実装\n",
    "本セクションでは、ニューラルネットワークの各層をPythonのクラスとして実装します。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 出力層 -回帰-\n",
    "以下は、回帰の場合の出力層を表すクラスです。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- 出力層 --\n",
    "class OutputLayer:\n",
    "    def __init__(self, n_upper, n):  # 初期設定\n",
    "        self.w = wb_width * np.random.randn(n_upper, n)  # 重み（行列）\n",
    "        self.b = wb_width * np.random.randn(n)  # バイアス（ベクトル）\n",
    "    \n",
    "    def forward(self, x):  # 順伝播\n",
    "        self.x = x\n",
    "        u = np.dot(x, self.w) + self.b\n",
    "        self.y = u  # 恒等関数\n",
    "    \n",
    "    def backward(self, t):  # 逆伝播\n",
    "        delta = self.y - t\n",
    "        \n",
    "        self.grad_w = np.dot(self.x.T, delta)\n",
    "        self.grad_b = np.sum(delta, axis=0)\n",
    "        \n",
    "        self.grad_x = np.dot(delta, self.w.T) \n",
    "\n",
    "    def update(self, eta):  # 重みとバイアスの更新\n",
    "        self.w -= eta * self.grad_w\n",
    "        self.b -= eta * self.grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`OutputLayer`クラスには4つのメソッドが定義されています。  \n",
    "\n",
    "コンストラクタ（`__init__`メソッド）は初期設定を行います。  \n",
    "`foward`メソッドは順伝播のメソッドです。  \n",
    "`backward`メソッドは逆伝播用のメソッドです。  \n",
    "`update`メソッドは重みとバイアスの更新用のメソッドです。  \n",
    " \n",
    "前のセクションでは関数として層を実装しましたが、クラスにすることで層がより機能的になっています。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 出力層 -分類-\n",
    "分類問題において、出力層は回帰とほぼ同じクラスで実装します。  \n",
    "唯一の違いは、出力層の活性化関数にソフトマックス関数を使用する点です。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OutputLayer:\n",
    "    def __init__(self, n_upper, n):  # 初期設定\n",
    "        self.w = wb_width * np.random.randn(n_upper, n)  # 重み（行列）\n",
    "        self.b = wb_width * np.random.randn(n)  # バイアス（ベクトル）\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        u = np.dot(x, self.w) + self.b\n",
    "        self.y = np.exp(u)/np.sum(np.exp(u), axis=1, keepdims=True)  # ソフトマックス関数\n",
    "        \n",
    "    def backward(self, t):  # 逆伝播\n",
    "        delta = self.y - t\n",
    "        \n",
    "        self.grad_w = np.dot(self.x.T, delta)\n",
    "        self.grad_b = np.sum(delta, axis=0)\n",
    "        \n",
    "        self.grad_x = np.dot(delta, self.w.T) \n",
    "\n",
    "    def update(self, eta):  # 重みとバイアスの更新\n",
    "        self.w -= eta * self.grad_w\n",
    "        self.b -= eta * self.grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum関数の引数に`keepdims=True`を設定することで、元の配列の次元が保たれます。  \n",
    "これにより、以下の箇所の計算結果は(バッチサイズ x 1)の行列になります。\n",
    "\n",
    "```Python\n",
    "np.sum(np.exp(u), axis=1, keepdims=True) \n",
    "```\n",
    "\n",
    "この計算結果と、`np.exp(u)`の計算結果は行の数がともにバッチサイズで一致しているので、Numpyのブロードキャスト機能を適用し、割り算を行うことができます。  \n",
    "これにより、バッチに対応したソフトマックス関数になります。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中間層\n",
    "以下は、回帰、分類共通の中間層を表すクラスです。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- 中間層 --\n",
    "class MiddleLayer:\n",
    "    def __init__(self, n_upper, n):  # 初期設定\n",
    "        self.w = wb_width * np.random.randn(n_upper, n)  # 重み（行列）\n",
    "        self.b = wb_width * np.random.randn(n)  # バイアス（ベクトル）\n",
    "\n",
    "    def forward(self, x):  # 順伝播\n",
    "        self.x = x\n",
    "        u = np.dot(x, self.w) + self.b\n",
    "        self.y = 1/(1+np.exp(-u))  # シグモイド関数\n",
    "    \n",
    "    def backward(self, grad_y):  # 逆伝播\n",
    "        delta = grad_y * (1-self.y)*self.y  # シグモイド関数の微分\n",
    "        \n",
    "        self.grad_w = np.dot(self.x.T, delta)\n",
    "        self.grad_b = np.sum(delta, axis=0)\n",
    "        \n",
    "        self.grad_x = np.dot(delta, self.w.T) \n",
    "        \n",
    "    def update(self, eta):  # 重みとバイアスの更新\n",
    "        self.w -= eta * self.grad_w\n",
    "        self.b -= eta * self.grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力層との違いは、活性化関数がシグモイド関数である点と、```backward```メソッドで```delta```を求めるのに出力層と異なる式を使用している点です。  \n",
    "このクラスを用いれば、例えば以下のように中間層をいくつでもインスンスとして生成することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "middle_layer_1 = MiddleLayer(3, 4)\n",
    "middle_layer_2 = MiddleLayer(4, 5)\n",
    "middle_layer_3 = MiddleLayer(5, 6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
