{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcX6BMvXT_UP"
   },
   "source": [
    "# 実用上のモデルチューニング\n",
    "本単元からは、実務でモデルのチューニングのために使われることの多いpytorchの様々な機能について紹介する。  \n",
    "ここまでの単元では扱いきれなかった、関数の詳細な使い方や、重要な活性化関数や損失関数、最適化関数を紹介する。  \n",
    "\n",
    "## この単元の目標\n",
    "- 様々な活性化関数を知ろう\n",
    "- 様々な損失関数を知ろう\n",
    "- 様々な最適化関数を知ろう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wopAJ8UOUNFO"
   },
   "source": [
    "## 1. 活性化関数によるモデルチューニング\n",
    "「relu関数」はすでに紹介済みだが、有名な活性化関数は他に「sigmoid関数」や「softmax関数」がある。  \n",
    "pytorchで実装されている、これらの活性化関数の使い方と挙動を復習も兼ねて確認しよう。  \n",
    "\n",
    "<br>\n",
    "\n",
    "**【relu関数】**  \n",
    "relu関数は、**「0未満の値を0に変換する」**という関数だ。0以上の値に対しては特に作用しない。  \n",
    "pytorchでは、`torch.nn.functional`モジュールで`relu()`関数として実装されている。  \n",
    "<br>\n",
    "\n",
    "**【softmax関数】**  \n",
    "softmax関数は、**「入力ベクトルの要素が0以上で、和が1になるようにスケール処理する」**という関数だ。  \n",
    "スケール処理なので、**要素ごとの大小関係は変わらない。**  \n",
    "使用例としては、MLPの出力層の出力にsoftmax関数を適用することで、その出力ベクトルは「モデルが予測した各クラスごとの確率」と言える。  \n",
    "pytorchでは`torch.nn.fuctional`モジュールで`softmax()`関数として実装されている。  \n",
    "本講座で扱ったような、**1次元のベクトルにsoftmax関数を適用する場合は、引数`dim`に`0`を指定する。**  \n",
    "`dim=2`以上の場合は複雑で、今すぐ必要な知識ではないので、説明は割愛する。  \n",
    "\n",
    "<br>\n",
    "\n",
    "**【sigmoid関数】**  \n",
    "「sigmoid」とかいて「シグモイド」と読む。  \n",
    "**「入力値を0から1に収まるように処理する」**という関数だ。  \n",
    "単調増加な関数で、入力値が小さいほど出力値は0に近くなり、大きいほど1に近くなる。  \n",
    "pytorchでは`torch`モジュールで`sigmoid()`関数として使用できる。  \n",
    "`torch.nn.functional`モジュールでも呼び出すことはできるが、公式的に前者を推奨されているため、そちらをおすすめする。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNTJjzK4eICk"
   },
   "source": [
    "【例題】 以下のコードを実行して、それぞれの活性化関数の挙動を確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 869,
     "status": "ok",
     "timestamp": 1590576129088,
     "user": {
      "displayName": "hiro ino",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihSdoqMmcaSzIVlnJIdtCgZPtPBEFqr0Y3_Cyp=s64",
      "userId": "16665494520159410568"
     },
     "user_tz": -540
    },
    "id": "_I23Q2LiePh3",
    "outputId": "dcc16c2c-098e-4102-f573-60f9d884930f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu: tensor([0., 1., 4., 0.])\n",
      "softmax: tensor([0.0023, 0.0465, 0.9341, 0.0171])\n",
      "sigmoid: tensor([0.1192, 0.7311, 0.9820, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.tensor([-2.0, 1.0, 4.0, 0.0])\n",
    "\n",
    "print('relu:', F.relu(x))\n",
    "print('softmax:', F.softmax(x, dim=0))\n",
    "print('sigmoid:', torch.sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q96a75iIgVEn"
   },
   "source": [
    "- ```\n",
    "relu: tensor([0., 1., 4., 0.])\n",
    "softmax: tensor([0.0023, 0.0465, 0.9341, 0.0171])\n",
    "sigmoid: tensor([0.1192, 0.7311, 0.9820, 0.5000])\n",
    "```\n",
    "と表示されていれば成功だ。\n",
    "- relu()関数の結果、0以下の値が全て0となっている。\n",
    "- softmax()関数の結果、値の大小関係は変わらずに、全ての値が正で、和が1となっている。\n",
    "- sigmoid()関数の結果、値の大小関係は変わらずに、全ての値が0から1に収まっている。  \n",
    "ちなみに、0を入力したときの出力は0.5となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0VL84n5eiEq9"
   },
   "source": [
    "【問題】 テンソル`[-5.0, 5.0, -10.0, 10.0]`に対して、3つの活性化関数を適用して出力を表示しよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1462,
     "status": "ok",
     "timestamp": 1590576089034,
     "user": {
      "displayName": "hiro ino",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihSdoqMmcaSzIVlnJIdtCgZPtPBEFqr0Y3_Cyp=s64",
      "userId": "16665494520159410568"
     },
     "user_tz": -540
    },
    "id": "lyrTeNDXiErE",
    "outputId": "84f78732-3a1f-4696-a89d-5f20c9c27f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu: tensor([ 0.,  5.,  0., 10.])\n",
      "softmax: tensor([3.0385e-07, 6.6928e-03, 2.0474e-09, 9.9331e-01])\n",
      "sigmoid: tensor([6.6929e-03, 9.9331e-01, 4.5398e-05, 9.9995e-01])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.tensor([-5.0, 5.0, -10.0, 10.0])\n",
    "\n",
    "print('relu:', F.relu(x))\n",
    "print('softmax:', F.softmax(x, dim=0))\n",
    "print('sigmoid:', torch.sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAIivALJiErM"
   },
   "source": [
    "- ```\n",
    "relu: tensor([ 0.,  5.,  0., 10.])\n",
    "softmax: tensor([3.0385e-07, 6.6928e-03, 2.0474e-09, 9.9331e-01])\n",
    "sigmoid: tensor([6.6929e-03, 9.9331e-01, 4.5398e-05, 9.9995e-01])\n",
    "```\n",
    "と表示されていれば成功だ。\n",
    "- relu()関数の結果、0以下の値が全て0となっている。\n",
    "- softmax()関数の結果、値の大小関係は変わらずに、全ての値が正で、和が1となっている。\n",
    "- sigmoid()関数の結果、値の大小関係は変わらずに、全ての値が0から1に収まっている。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fONEmtECUO1t"
   },
   "source": [
    "## 2. 損失関数によるモデルチューニング\n",
    "次に扱うのは、損失関数だ。  \n",
    "損失関数は、クラスとして実装されているものをインスタンスで宣言したのち、  \n",
    "損失計算するタイミングで`__call__()`関数を呼び出す方法が一般的だ。  \n",
    "さて、頻繁に用いられる損失関数`CrossEntropyLoss()`関数を新たに加えた、様々な損失関数を学ぼう。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fONEmtECUO1t"
   },
   "source": [
    "**【nn.MSELoss】**  \n",
    "MSELossは最も主流な損失関数の一つだ。  \n",
    "2つのベクトルの**要素ごとの差の2乗**から誤差を算出する。  \n",
    "pytorchでは、`torch.nn`モジュールで`MSELoss()`としてクラスが定義されている。  \n",
    "インスタンス宣言時に、引数`reduction='mean'`で**要素ごとの誤差の平均**、`reduction='sum'`で**合計**を損失値として出力する。  \n",
    "`reduction='none'`を指定すると、平均や合計処理をする前の、**要素ごとの誤差をテンソルとして**出力する。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fONEmtECUO1t"
   },
   "source": [
    "**【nn.CrossEntropyLoss】**  \n",
    "CrossEntropyLoss（以下：CE）もかなり頻繁に使用される損失関数で、  \n",
    "MSEが「全要素の誤差」を求めていたのに対し、こちらは**「正解ラベルと対応する出力との差のみ」**を計算する。\n",
    "しかし、正しくこれを用いるためには、**モデルの出力をsoftmax関数などで確率にする必要**がある。  \n",
    "なぜならば、正解ラベルに対するモデルの出力が「1」のときに損失が0となってしまうからだ。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fONEmtECUO1t"
   },
   "source": [
    "以下の例を見てほしい。  \n",
    "【例：正解ラベルが「1」（one-hotベクトルで`[0, 1, 0]`）  】 →**モデルの出力ベクトルの2番目の要素と比較する**\n",
    "- モデルの出力: `[0.2 0.2 0.6]`（確率ベクトル） → 損失=**0.2と1との差**\n",
    "- モデルの出力: `[1.0 1.0 3.0]`（確率ベクトルでない） → 損失=**1と1との差**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fONEmtECUO1t"
   },
   "source": [
    "MSEと違いCEは、正解ラベルと対応する出力の誤差 **\"のみ\"** を計算する。  \n",
    "2つ目の場合では、「1と1との損失」を計算するが、当然ながらこれは**「0」**となってしまい、これ以上最適化できない。  \n",
    "つまり、正解ラベルとの比較対象が確率ベクトルになっていないと、**正しく損失が計算されない**可能性がある。  \n",
    "定義式など、CEの詳細な説明はここでは割愛するが、**「モデルの出力を確率ベクトルに変換する必要がある」**ということは覚えておこう。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fONEmtECUO1t"
   },
   "source": [
    "CEを使用する上で確率値に変換しておく必要があるが、pytorchでは確率値に変換する`softmax()`を組み込んだ`nn.CrossEntropyLoss()`としてクラス定義されている。 \n",
    "\n",
    "このため、CEを使用する際にわざわざsoftmax関数で確率値に変換する必要はない。\n",
    "\n",
    "`nn.MSE()`と同様に`({予測値}, {正解ラベル})`という形式で引数を与えるが、  \n",
    "正解ラベルは**one-hotベクトルではなく、正解ラベルを表す値をそのまま与える。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fONEmtECUO1t"
   },
   "source": [
    "さらに，CEの派生でBinaryCrossEntropyLossというものがある。  \n",
    "2クラス分類用に定義された損失関数で、pytorchには`nn.BCELoss`として実装してある。  \n",
    "詳細は、余裕があればぜひ調べてみてほしい。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-VTmZSSaVq8w"
   },
   "source": [
    "【例題】 以下のコードを実行して、それぞれの損失関数の挙動を確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1258,
     "status": "ok",
     "timestamp": 1590575723583,
     "user": {
      "displayName": "hiro ino",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihSdoqMmcaSzIVlnJIdtCgZPtPBEFqr0Y3_Cyp=s64",
      "userId": "16665494520159410568"
     },
     "user_tz": -540
    },
    "id": "qF7u6gA6nBnY",
    "outputId": "67971e71-d2c6-4fe4-e6d2-42aac6dacfca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: tensor(9.6667)\n",
      "CrossEntropy: tensor(0.1698)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# バッチサイズ=1と見立てたデータを用意\n",
    "x = torch.tensor([[2., 5., 3.]])\n",
    "mse_label = torch.tensor([[0, 1, 0]])\n",
    "cel_label = torch.tensor([1])\n",
    "\n",
    "print('MSE:', nn.MSELoss(reduction='mean')(x, mse_label))  ## インスタンス宣言と__call__()を同時に呼び出している\n",
    "print('CrossEntropy:', nn.CrossEntropyLoss()(x, cel_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lo3Vue0rVCfn"
   },
   "source": [
    "- ```\n",
    "MSE: tensor(0.1267)\n",
    "CrossEntropy: tensor(0.9398)\n",
    "```\n",
    "と表示されていれば成功だ。\n",
    "- 異なる種類の損失値は、比較できないので注意しよう。  \n",
    "この場合、`x`も正解データも同じものを与えているが「CE>MSE」となっている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wl5-sbzjqDKa"
   },
   "source": [
    "【問題】 テンソル`x = torch.tensor([[0.3, 0.3, 0.3, 0.1]])`に対して正解ラベルを3としてMSEとCEを使って損失を計算しよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1113,
     "status": "ok",
     "timestamp": 1590575994666,
     "user": {
      "displayName": "hiro ino",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihSdoqMmcaSzIVlnJIdtCgZPtPBEFqr0Y3_Cyp=s64",
      "userId": "16665494520159410568"
     },
     "user_tz": -540
    },
    "id": "7T8jIZNHqDKc",
    "outputId": "f7c7a4b0-7d14-4a74-ff2b-e05502150087"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: tensor(0.2700)\n",
      "CrossEntropy: tensor(1.5399)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# バッチサイズ=1と見立てたデータを用意\n",
    "x = torch.tensor([[0.3, 0.3, 0.3, 0.1]])\n",
    "mse_label = torch.tensor([[0, 0, 0, 1]])\n",
    "ce_label = torch.tensor([3])\n",
    "\n",
    "print('MSE:', nn.MSELoss(reduction='mean')(x, mse_label))  ## インスタンス宣言と__call__()を同時に呼び出している\n",
    "print('CrossEntropy:', nn.CrossEntropyLoss()(x, ce_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFY8wwVEqDKi"
   },
   "source": [
    "- ```\n",
    "MSE: tensor(0.2700)\n",
    "CrossEntropy: tensor(1.5399)\n",
    "```\n",
    "と表示されていれば成功だ。\n",
    "- 例題と同じように「CE>MSE」となっているが、常に「CE>MSE」と言う訳ではない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hsj3ldvUR9u"
   },
   "source": [
    "## 3. 最適化関数によるモデルチューニング\n",
    "最後は、最適化関数について学ぼう。  \n",
    "最適化関数も損失関数と同様に、一般的には「インスタンスを作成→`__call__()`を呼び出す」という流れで実行する。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hsj3ldvUR9u"
   },
   "source": [
    "**【SGD】**  \n",
    "最適化関数の説明をするときに、一番イメージしやすくシンプルなのは**SGD**だろう。  \n",
    "SGDの正式名称は「Stochastic Gradient Descent」といい、日本語では**「確率的勾配降下法」**と言う。  \n",
    "「勾配降下法」は「逆伝播」の単元で説明した通り、勾配から損失を最小化するようにパラメータを更新する事だが、SGDは「確率的」にこれを行う。  \n",
    "何が確率的かというと、勾配を全データに対してではなく**ランダムサンプルしたデータに対して計算し、最適化を行う**のだ。  \n",
    "全データに対して計算される勾配は一意に定まるが、**ランダムサンプルした場合の勾配はデータによって変化する**性質を利用している。  \n",
    "pytorchでは、`torch.optim`モジュールの`SGD()`としてクラスが定義されている。  \n",
    "学習率は、自動で設定されないので、**引数`lr`で明示的に指定する必要がある。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hsj3ldvUR9u"
   },
   "source": [
    "**【Momentum】**  \n",
    "**Momentum**は、SGDを改良した最適化関数で、日本語で「運動量」や「勢い」と訳す。  \n",
    "その名の通り、SGDに現在の勾配だけでなく**過去の勾配情報を追加**させることで、**「勢い」を考慮した最適化**を行う。  \n",
    "pytorchでは、`Momentum()`というクラスはなく、`SGD()`の引数`momentum`を指定することで宣言できる。  \n",
    "引数`momentum`は標準で`0（勢いを考慮しない）`で、使用する場合はこれに数値を指定する。  \n",
    "例えば、前回の勾配を半分だけ考慮したい場合には`0.5`、1割だけ考慮したい場合は`0.1`だ。  \n",
    "下式を見てもらうとわかるが、`momentum>1`だと過去の勾配をより強く考慮してしまうため正しく最適化できない可能性が高い。\n",
    "\n",
    "今回の勾配を $ d_t $　、前回の勾配を $ d_{t-1} $ として、\n",
    "\n",
    "$$ 勾配を求める式: (勾配) = d_t + momentum × d_{t-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hsj3ldvUR9u"
   },
   "source": [
    "**【Adam】**  \n",
    "新しい手法ではないが、現在でも主流な最適化関数が**Adam**だ。  \n",
    "Adamは、Momentumに、**「勾配の大きさに応じて学習率`lr`を調節する機能」**を追加した最適化関数だ。  \n",
    "今回紹介している3つの最適化関数は、SGD→Momentum→Adamという順に進化している。  \n",
    "ご存知の通り、pytorchでは`torch.optim`モジュールに`Adam()`としてクラスが定義してある。  \n",
    "`Adam()`には、`lr`の他に`betas`、`eps`、`weight_decay`、`amsgrad`といった様々なハイパーパラメータが引数として指定できる。  \n",
    "かなり難しい内容なのでここでは扱わないが、興味があれば調べてみてほしい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B3JOKZ13VzIE"
   },
   "source": [
    "【例題】 以下のコードを実行して、それぞれの最適化関数を宣言する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3924,
     "status": "ok",
     "timestamp": 1590576276555,
     "user": {
      "displayName": "hiro ino",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihSdoqMmcaSzIVlnJIdtCgZPtPBEFqr0Y3_Cyp=s64",
      "userId": "16665494520159410568"
     },
     "user_tz": -540
    },
    "id": "8YIeuziLV4nB",
    "outputId": "8f8bf2e6-3e86-49fd-9b47-24eadb799f83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adam: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "sgd: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "momentum: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from torchvision.models import vgg11\n",
    "\n",
    "model = vgg11()\n",
    "\n",
    "adam = optim.Adam(model.parameters())\n",
    "sgd = optim.SGD(model.parameters(), lr=0.01)\n",
    "momentum = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "print('adam:',adam)\n",
    "print('sgd:',sgd)\n",
    "print('momentum:',momentum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N89oa2H-VuPL"
   },
   "source": [
    "- ```\n",
    "adam: Adam (\n",
    "Parameter Group 0\n",
    "    amsgrad: False\n",
    "    betas: (0.9, 0.999)\n",
    "    eps: 1e-08\n",
    "    lr: 0.001\n",
    "    weight_decay: 0\n",
    ")\n",
    "sgd: SGD (\n",
    "Parameter Group 0\n",
    "    dampening: 0\n",
    "    lr: 0.01\n",
    "    momentum: 0\n",
    "    nesterov: False\n",
    "    weight_decay: 0\n",
    ")\n",
    "momentum: SGD (\n",
    "Parameter Group 0\n",
    "    dampening: 0\n",
    "    lr: 0.01\n",
    "    momentum: 0.9\n",
    "    nesterov: False\n",
    "    weight_decay: 0\n",
    ")\n",
    "```\n",
    "と表示されていれば成功だ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEbC_9PPulQP"
   },
   "source": [
    "【問題】 例題のコードから、3つの最適化関数の引数`lr`を`0.2`に変更して宣言し、例題と同じように出力してみよう。  \n",
    "第1引数のモデルのパラメータは`vgg11()`のものを使おう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3614,
     "status": "ok",
     "timestamp": 1590576336197,
     "user": {
      "displayName": "hiro ino",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihSdoqMmcaSzIVlnJIdtCgZPtPBEFqr0Y3_Cyp=s64",
      "userId": "16665494520159410568"
     },
     "user_tz": -540
    },
    "id": "z5ghdBjDulQR",
    "outputId": "753765cb-e0b9-4861-9fe8-aba3a2582de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adam: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.2\n",
      "    weight_decay: 0\n",
      ")\n",
      "sgd: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.2\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "momentum: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.2\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = vgg11()\n",
    "\n",
    "adam = optim.Adam(model.parameters(), lr=0.2)\n",
    "sgd = optim.SGD(model.parameters(), lr=0.2)\n",
    "momentum = optim.SGD(model.parameters(), lr=0.2, momentum=0.9)\n",
    "\n",
    "print('adam:',adam)\n",
    "print('sgd:',sgd)\n",
    "print('momentum:',momentum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WK9HHfOfulQW"
   },
   "source": [
    "- ```\n",
    "adam: Adam (\n",
    "Parameter Group 0\n",
    "    amsgrad: False\n",
    "    betas: (0.9, 0.999)\n",
    "    eps: 1e-08\n",
    "    lr: 0.2\n",
    "    weight_decay: 0\n",
    ")\n",
    "sgd: SGD (\n",
    "Parameter Group 0\n",
    "    dampening: 0\n",
    "    lr: 0.2\n",
    "    momentum: 0\n",
    "    nesterov: False\n",
    "    weight_decay: 0\n",
    ")\n",
    "momentum: SGD (\n",
    "Parameter Group 0\n",
    "    dampening: 0\n",
    "    lr: 0.2\n",
    "    momentum: 0.9\n",
    "    nesterov: False\n",
    "    weight_decay: 0\n",
    ")\n",
    "```\n",
    "と表示されていれば成功だ。\n",
    "- 例題の出力と比較して、`lr`の値が`0.2`と変わっていることを確認しよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SHLsfkHQe49W"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "7_pytorchの様々な機能1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
