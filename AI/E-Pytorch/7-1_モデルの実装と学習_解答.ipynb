{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xbkUbPgQPKWw"
   },
   "source": [
    "# モデルの実装と学習\n",
    "この単元では、pytorchを使った**MLPの実装方法**を扱う。  \n",
    "具体的には、モデルの要素となる**全結合層や活性化関数、損失関数、最適化関数**を実装し、それらを組み合わせた**モデルを定義**する。\n",
    "\n",
    "これまでで学んだMLPの理論を思い出しつつ、この単元では、MLPモデルの実装方法を学んでいこう！  \n",
    "この章でモデルの実装方法を学んで、次の章からはモデルを学習していくぞ！\n",
    "\n",
    "## この単元の目標\n",
    "- 全結合層と活性化関数ReLUの実装方法を学ぶ\n",
    "- 損失関数と最適化関数の実装方法を学ぶ\n",
    "- MLPモデルの実装方法を学ぶ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I9b-7HGgKBrH"
   },
   "source": [
    "## 1. 全結合層と活性化関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RwjcVEfGFTsl"
   },
   "outputs": [],
   "source": [
    "## 本章から必要なモジュールのインポート\n",
    "import torch\n",
    "from torch import nn # 全結合層のため\n",
    "from torch.nn import functional as F # 活性化関数のため"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mj75fJUPUk51"
   },
   "source": [
    "復習になるが、MLPを構成する層は**「全結合層(Fully Connected Layer)」**と呼ばれる。  \n",
    "pytorchでは、全結合層は`nn.Linear()`クラスでインスタンスとして宣言できる。  \n",
    "第1引数`in_features`で入力となるノードの数（テンソルのサイズ）、第2引数`out_features`で出力となるノードの数を指定する必要がある。  \n",
    "また、伝播させるときは`__call__()`メソッドの引数にデータを与えて実行する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_vSGtIiWRSI"
   },
   "source": [
    "【例題1】 全結合層`fc`を定義して、テンソル`[1,2,3,4]`を伝播させる。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 496,
     "status": "ok",
     "timestamp": 1590144961738,
     "user": {
      "displayName": "hiro ino",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihSdoqMmcaSzIVlnJIdtCgZPtPBEFqr0Y3_Cyp=s64",
      "userId": "16665494520159410568"
     },
     "user_tz": -540
    },
    "id": "iAOTv2lFVmsK",
    "outputId": "cd6db1ce-2c53-4284-8254-08da3a889f05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.8078, 0.7770], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 4次元ベクトルを2次元に変換する全結合層をクラスとして定義\n",
    "fc = nn.Linear(4, 2)\n",
    "x = torch.Tensor([1,2,3,4])\n",
    "\n",
    "# fcの__call__()メソッドを呼ぶことでxを伝播できる\n",
    "x = fc(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1MLM1bpffs_R"
   },
   "source": [
    "- `tensor([x, y], grad_fn=<AddBackward0>)`と表示できていれば成功だ。  \n",
    "`［x, y］`には，任意の小数が入る．\n",
    "- `x, y`は`[1,2,3,4]`を`fc_layer`の初期パラメータで変換した値だ。\n",
    "- 4次元テンソルが2次元に変換されていることを確認しよう。\n",
    "- `grad_fn=<AddBackward0>`はパラメータだ。損失を求めるために使う。\n",
    "- `fc`をインスタンスとして宣言し、`__call__()`で伝播させる流れを理解しよう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqynUfgTaYni"
   },
   "source": [
    "次は、**「活性化関数」**の実装方法を学ぼう。  \n",
    "pytorchでは，`torch.nn.functional`モジュールに活性化関数が**「関数」**として用意されているので、これを使えばいい。\n",
    "\n",
    "今回実装するのは、ReLU関数だ。  \n",
    "これは、入力データに対して「0以下の値を0に変換」する機能を持った活性化関数だ。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B_JkRdYVdZl0"
   },
   "source": [
    "【例題2】 `relu()`関数に、テンソル`[-1.0, -0.5, 0.5, 1.0]`を与えて出力を確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 797,
     "status": "ok",
     "timestamp": 1590144969247,
     "user": {
      "displayName": "hiro ino",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihSdoqMmcaSzIVlnJIdtCgZPtPBEFqr0Y3_Cyp=s64",
      "userId": "16665494520159410568"
     },
     "user_tz": -540
    },
    "id": "WZvjovwhaf6c",
    "outputId": "86c09206-ca76-4fdb-e325-99abb04bb260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0000, 0.5000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([-1.0, -0.5, 0.5, 1.0])\n",
    "x = F.relu(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mxcuSsfXjPJj"
   },
   "source": [
    "- `tensor([0.0000, 0.0000, 0.5000, 1.0000])`と表示されていれば成功だ。\n",
    "- 0以下の値が「0.0000」となっていることを確認しておこう。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6jhfBA5pAgLP"
   },
   "source": [
    "【問題】 テンソル`[-2, -1, 1, 2]`を`nn.Linear()`と`F.relu()`のそれぞれに伝播した結果を表示しよう。  \n",
    "全結合層の出力ノード数は2とする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 542,
     "status": "ok",
     "timestamp": 1590148032803,
     "user": {
      "displayName": "hiro ino",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihSdoqMmcaSzIVlnJIdtCgZPtPBEFqr0Y3_Cyp=s64",
      "userId": "16665494520159410568"
     },
     "user_tz": -540
    },
    "id": "Xe_d4usaAxjW",
    "outputId": "2666fed9-2ec6-4e9e-8ef6-454fab13461b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0651, -0.1810], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 1., 2.])\n"
     ]
    }
   ],
   "source": [
    "# xとfcの用意\n",
    "x = torch.Tensor([-2, -1, 1, 2])\n",
    "fc = nn.Linear(4, 2)\n",
    "\n",
    "# 全結合層を通したxを表示\n",
    "print(fc(x))\n",
    "\n",
    "# さらに，relu関数を適用したxを表示\n",
    "print(F.relu(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LVCvTRXeNyKP"
   },
   "source": [
    "- 全結合層の出力: `tensor([XXXX, XXXX], grad_fn=<AddBackward0>)`\n",
    "- `relu()`の出力: `tensor([0., 0., 1., 2.])`  \n",
    "のように表示されていれば正解だ。\n",
    "- 全結合層の出力となるテンソルの要素は、どんな値でもいい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLMBz0Xp87y5"
   },
   "source": [
    "## 2. 損失関数と最適化関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-7DxJgEd9A6p"
   },
   "outputs": [],
   "source": [
    "## 本章から必要なモジュールのインポート\n",
    "from torch import optim\n",
    "from torchvision.models import vgg11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "89W2Hb0CVA1Y"
   },
   "source": [
    "損失関数と最適化関数の実装方法について学んでいこう。  \n",
    "損失関数と最適化関数は、全結合層`fc`のように、  \n",
    "**「インスタンスを宣言」→「`__call__()`を実行」**  \n",
    "という流れで使用する。\n",
    "\n",
    "まずは、**損失関数**だ。  \n",
    "pytorchでは、第1章でインポートした`torch.nn`モジュールで損失関数が用意されている。  \n",
    "\n",
    "今回はMSE(平均二乗誤差)を実装する。\n",
    "MSEは`torch.nn.MSELoss()`というクラスで定義されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9K4_sEqLjPbt"
   },
   "source": [
    "【例題1】 テンソル`[0,1,2]`とテンソル`[1,-1,0]`のMSEを求める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1373,
     "status": "ok",
     "timestamp": 1590210917921,
     "user": {
      "displayName": "hiro ino",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihSdoqMmcaSzIVlnJIdtCgZPtPBEFqr0Y3_Cyp=s64",
      "userId": "16665494520159410568"
     },
     "user_tz": -540
    },
    "id": "5CZf9Na6QUP8",
    "outputId": "e1677be2-f065-48b7-dd7e-16b60ff3e2d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "# まず、損失関数をクラスとして用意する\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "x = torch.Tensor([0,1,2])\n",
    "y = torch.Tensor([1,-1,0])\n",
    "\n",
    "# 用意したクラスの__call__()を実行\n",
    "loss = criterion(x, y)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KNFeADMHQSue"
   },
   "source": [
    "- `tensor(3.)`と表示されていれば成功だ。\n",
    "- 引数として与えられた2つのテンソルの損失を計算している。\n",
    "- 要素ごとの差の2乗の平均値となっていることを確認しよう。（下式）\n",
    "$$ \\frac{(0-1)^{2} + (1-(-1))^2 + (2-0)^2}{3} = 3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-hrIOsMRpsz"
   },
   "source": [
    "次は、**最適化関数**の実装方法だ。  \n",
    "pytorchでは`torch.optim`モジュールに最適化関数が用意されている。\n",
    "\n",
    "最適化関数のインスタンスを宣言するときには、**最適化するモデルのパラメータ**を引数に与える必要があるので、  例として`vgg11`というモデルを用意する。\n",
    "\n",
    "今回は`Adam`という最適化関数を実装するぞ。  \n",
    "これは、今でも**頻繁に使われる**実用的なものなので、覚えておこう。  \n",
    "`Adam`は`torch.optim.Adam()`で定義されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VXYJAdva6rc8"
   },
   "source": [
    "【例題2】 `Adam()`のインスタンスを宣言する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2996,
     "status": "ok",
     "timestamp": 1590212471386,
     "user": {
      "displayName": "hiro ino",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihSdoqMmcaSzIVlnJIdtCgZPtPBEFqr0Y3_Cyp=s64",
      "userId": "16665494520159410568"
     },
     "user_tz": -540
    },
    "id": "Lydm7O3p85z_",
    "outputId": "10629d10-3bed-478f-b46c-433d178d5e2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# モデルを用意する\n",
    "model = vgg11()\n",
    "\n",
    "# 用意したモデルのパラメータを与えて、クラスを定義する。\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VPg-5GP0DCSg"
   },
   "source": [
    "- ```\n",
    "Adam (\n",
    "Parameter Group 0\n",
    "    amsgrad: False\n",
    "    betas: (0.9, 0.999)\n",
    "    eps: 1e-08\n",
    "    lr: 0.001\n",
    "    weight_decay: 0\n",
    ")\n",
    "```\n",
    "と表示されていれば成功だ。\n",
    "- `lr`は学習率のことで、標準で「0.001」だが、これは引数で変更する事ができる。\n",
    "- `amsgrad`や`betas`などもハイパーパラメータだ。余裕があれば調べてみよう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EqMshXuIDOPY"
   },
   "source": [
    "【問題】 損失関数MSEを宣言して、`[1,1,1,1]`と`[0,2,4,6]`の損失を計算、表示してみよう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 605,
     "status": "ok",
     "timestamp": 1590370820304,
     "user": {
      "displayName": "hiro ino",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihSdoqMmcaSzIVlnJIdtCgZPtPBEFqr0Y3_Cyp=s64",
      "userId": "16665494520159410568"
     },
     "user_tz": -540
    },
    "id": "MZa0_jplDnI1",
    "outputId": "150e7a7d-ad8c-437a-9a1d-b49bed8fab0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "x = torch.Tensor([1,1,1,1])\n",
    "y = torch.Tensor([0,2,4,6])\n",
    "\n",
    "loss = criterion(x, y)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0oHE_40jfar1"
   },
   "source": [
    "- `tensor(9.)`と表示されていれば成功だ。\n",
    "- 要素ごとの差の2乗の平均値となっていることを確認しよう。（下式）\n",
    "$$ \\frac{(1-0)^{2} + (1-2)^2 + (1-4)^2 + (1-6)^2}{4} = 9$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-i12dp8CdJJS"
   },
   "source": [
    "## 3. モデルの定義\n",
    "初期化関数`__init__()`で、モデルに使うレイヤーや関数を準備し、`forward()`関数でモデルを組み立てる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-DrXZAj2CmJu"
   },
   "source": [
    "【例題】 以下の条件のMLPモデルをクラスとして定義する。  \n",
    "- 「入力層、中間層、出力層」のノードの数が「3、5、2」\n",
    "- 中間層の出力に活性化関数`relu()`を適用\n",
    "- 損失関数は`MSELoss()`\n",
    "- 最適化関数は`Adam()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O_tuQGItCHCz"
   },
   "outputs": [],
   "source": [
    "class mlp_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 全結合層を2つ\n",
    "        self.fc1 = nn.Linear(3, 5)\n",
    "        self.fc2 = nn.Linear(5, 2)\n",
    "\n",
    "        # 損失関数と最適化関数\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        print('[fc1を通過]\\n', x) # 中間層の出力を表示\n",
    "        # 活性化関数\n",
    "        x = F.relu(x)\n",
    "        print('[relu()を通過]\\n', x) # relu()の適用結果を表示\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zvgCDNkqFG-g"
   },
   "source": [
    "- モデルクラスを自作するときは`nn.Module`を継承することを忘れないようにしよう。\n",
    "- 初期化関数`__init__()`について\n",
    "  - `super().__init__()`により、`nn.Module`の初期化関数を適用する。\n",
    "  - `forward()`で使用する全結合層と、学習時に使用する損失関数・最適化関数をここで宣言する。\n",
    "- 順伝播を行う`forward()`について\n",
    "  - `forward()`は`nn.Module`を継承したことにより`__call__()`メソッドで呼ばれる。\n",
    "  - ここで、「fc1→relu→fc2」という構造を組み立てている。\n",
    "  - `x`の値が層を伝播するごとに変化することを認識しよう。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uQzp5vYBJN-r"
   },
   "source": [
    "作成したモデルに、テンソル`[0,1,2]`を伝播させてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1258,
     "status": "ok",
     "timestamp": 1590214363387,
     "user": {
      "displayName": "hiro ino",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihSdoqMmcaSzIVlnJIdtCgZPtPBEFqr0Y3_Cyp=s64",
      "userId": "16665494520159410568"
     },
     "user_tz": -540
    },
    "id": "Gd8goyvKCksv",
    "outputId": "4d4b05f4-1bf9-4b08-f9f9-85d3d2e23c60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fc1を通過]\n",
      " tensor([-0.9387, -0.2004, -0.5228, -0.0900,  0.1645], grad_fn=<AddBackward0>)\n",
      "[relu()を通過]\n",
      " tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.1645], grad_fn=<ReluBackward0>)\n",
      "[モデルの出力]\n",
      " tensor([-0.1795, -0.2170], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# モデルを宣言\n",
    "model = mlp_net()\n",
    "\n",
    "x = torch.Tensor([0, 1, 2])\n",
    "\n",
    "# xを伝播させる\n",
    "output = model(x)\n",
    "\n",
    "print('[モデルの出力]\\n', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K-sNlgLLESLj"
   },
   "source": [
    "- ```\n",
    "[fc1を通過]\n",
    " tensor( {要素が5つのテンソル} , grad_fn=<AddBackward0>)\n",
    "[relu()を通過]\n",
    " tensor( {要素が5つのテンソル} , grad_fn=<ReluBackward0>)\n",
    "[モデルの出力]\n",
    " tensor( {要素が2つのテンソル} , grad_fn=<AddBackward0>)\n",
    "```  \n",
    "のように表示されているはずだ。\n",
    "- fc1, fc2の層ではテンソルの形状が変換され、`relu()`の出力では「0以下の要素が全て0」となっている事を確認しよう。\n",
    "- モデルの学習を行うときには、この`output`と理想の結果から`self.criterion`と`self.optimizer`を使用して最適化を行う。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pz2NMr7xIYfJ"
   },
   "source": [
    "【問題】  \n",
    "例題を参考に、以下の条件のMLPモデルをクラスとして定義してみよう。  \n",
    "その後、定義したモデルを出力してみよう（例: `class model():`→`model()`で出力）\n",
    "- 「入力層、中間層、出力層」のノードの数が「4、3、2」\n",
    "- 中間層の出力に活性化関数`relu()`を適用\n",
    "- 損失関数は`MSELoss()`\n",
    "- 最適化関数は`Adam()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eSj8YtGDIYfN"
   },
   "outputs": [],
   "source": [
    "class mlp_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 全結合層を2つ\n",
    "        self.fc1 = nn.Linear(4, 3)\n",
    "        self.fc2 = nn.Linear(3, 2)\n",
    "\n",
    "        # 損失関数と最適化関数\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        # 活性化関数\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1590213886966,
     "user": {
      "displayName": "hiro ino",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GihSdoqMmcaSzIVlnJIdtCgZPtPBEFqr0Y3_Cyp=s64",
      "userId": "16665494520159410568"
     },
     "user_tz": -540
    },
    "id": "wE1arUF5DeJK",
    "outputId": "4a192339-ca9b-4624-8dee-149e9a2d43d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlp_net(\n",
       "  (fc1): Linear(in_features=4, out_features=3, bias=True)\n",
       "  (fc2): Linear(in_features=3, out_features=2, bias=True)\n",
       "  (criterion): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jn7aI0wwmJaI"
   },
   "source": [
    "- ```\n",
    "mlp_net(\n",
    "  (fc1): Linear(in_features=4, out_features=3, bias=True)\n",
    "  (fc2): Linear(in_features=3, out_features=2, bias=True)\n",
    "  (criterion): MSELoss()\n",
    ")\n",
    "```\n",
    "のように表示されていれば成功だ。\n",
    "- fc1は、全結合層で入力ノードが4つ、出力ノードが3つである。\n",
    "- fc2は、全結合層で入力ノードが3つ、出力ノードが2つである。\n",
    "- criterionは、損失関数であり、定義されているのはMSELoss()である。\n",
    "- 最適化関数は表示されていないが、`{クラス名}.optimizer`で出力できる。  \n",
    "（例：`mlp_net().optimizer`）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EBMD2wiNZVah"
   },
   "source": [
    "## まとめ\n",
    "次章のモデルの学習に向けて以下の項目の内容を学んだ。\n",
    "- 全結合層と活性化関数ReLUの実装方法を学ぶ\n",
    "- 損失関数と最適化関数の実装方法を学ぶ\n",
    "- MLPモデルの実装方法を学ぶ\n",
    "\n",
    "ハイパーパラメータの一つである**モデルの構造**の定義方法について学んだ。\n",
    "\n",
    "## 次単元の内容\n",
    "次単元では、個々まで学んだことを全て利用して、モデルの学習・予測を行うぞ！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y8JGA5fZlq-i"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "6_モデルの実装.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
