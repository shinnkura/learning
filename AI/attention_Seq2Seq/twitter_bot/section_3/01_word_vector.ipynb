{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_word_vector.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyObz7r6fhB5XVa7+q8gkB64",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukinaga/twitter_bot/blob/master/section_3/01_word_vector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpqhL1fLlLsu"
      },
      "source": [
        "# 分散表現の実装\n",
        "ニューラルネットワークで扱いやすいように、日本語の文章を単語ベクトルの並びで表します"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbeonHALP8fU"
      },
      "source": [
        "### ライブラリののインストール\n",
        "形態素解析のためにjanomeをインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUm6Vi2wQBfa"
      },
      "source": [
        "!pip install janome==0.4.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGWOKNTUlYa3"
      },
      "source": [
        "## 形態素解析\n",
        "janomeを利用して、文章を形態素解析します。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmM9kOAgPTlB"
      },
      "source": [
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "print(\"文章を入力してください。\")\n",
        "text = input()  # ユーザーの入力を取得\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "for token in tokenizer.tokenize(text):\n",
        "    print(token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPRR4mLjtGC4"
      },
      "source": [
        "## データの読み込み\n",
        "今回は、夏目漱石の「我輩は猫である」の全文を読み込みます。  \n",
        "`wagahaiwa_nekodearu.txt`のアップロードが必要です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO9igZ2ltGC5"
      },
      "source": [
        "with open(\"wagahaiwa_nekodearu.txt\", mode=\"r\", encoding=\"utf-8\") as f:  # ファイルの読み込み\n",
        "    wagahai_original = f.read()\n",
        "\n",
        "print(wagahai_original)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAVRzlmxtGC8"
      },
      "source": [
        "## 文章の前処理\n",
        "読み込んだ小説のデータから、正規表現を用いてルビなどを削除します。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt7aUHgDtGC9"
      },
      "source": [
        "import re\n",
        "\n",
        "wagahai = re.sub(\"《[^》]+》\", \"\", wagahai_original) # ルビの削除\n",
        "wagahai = re.sub(\"［[^］]+］\", \"\", wagahai) # 読みの注意の削除\n",
        "wagahai = re.sub(\"[｜ 　「」\\n]\", \"\", wagahai) # | と全角半角スペース、「」と改行の削除\n",
        "\n",
        "seperator = \"。\"  # 。をセパレータに指定\n",
        "wagahai_list = wagahai.split(seperator)  # セパレーターを使って文章をリストに分割する\n",
        "wagahai_list.pop() # 最後の要素は空の文字列になるので、削除\n",
        "wagahai_list = [x+seperator for x in wagahai_list]  # 文章の最後に。を追加\n",
        "        \n",
        "print(wagahai_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mgjEpfv39bG"
      },
      "source": [
        "Janomeを使って分かち書きを行います。  \n",
        "分かち書きとは、文章を単語ごとに分割することです。  \n",
        "`tokenize`の際に引数を`wakati=True`にすることで、各単語をリストに格納できます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_daqxjw3e2w"
      },
      "source": [
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "wagahai_words = []\n",
        "for sentence in wagahai_list:\n",
        "    wagahai_words.append(list(tokenizer.tokenize(sentence, wakati=True)))   # 文章ごとに単語に分割し、リストに格納\n",
        "\n",
        "print(wagahai_words[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMckKokEtmxa"
      },
      "source": [
        "## word2vecを用いた学習\n",
        "今回はword2vecのためにライブラリgensimを使います。  \n",
        "https://radimrehurek.com/gensim/\n",
        "\n",
        "以下では、word2vecを用いてコーパスの学習を行い、学習済みのモデルを作成します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LBgkUsRtmxa"
      },
      "source": [
        "from gensim.models import word2vec\n",
        "\n",
        "# size : 中間層のニューロン数\n",
        "# min_count : この値以下の出現回数の単語を無視\n",
        "# window : 対象単語を中心とした前後の単語数\n",
        "# iter : epochs数\n",
        "# sg : skip-gramを使うかどうか 0:CBOW 1:skip-gram\n",
        "model = word2vec.Word2Vec(wagahai_words,\n",
        "                          size=100,\n",
        "                          min_count=5,\n",
        "                          window=5,\n",
        "                          iter=20,\n",
        "                          sg = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmmiFL7itmxc"
      },
      "source": [
        "分散表現を見ていきましょう。  \n",
        "重みを表す行列（分散表現）の形状と、行列そのものを表示します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REPbs0oItmxd"
      },
      "source": [
        "print(model.wv.vectors.shape)  # 分散表現の形状\n",
        "print(model.wv.vectors)  # 分散表現"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEwHKYf-tmxf"
      },
      "source": [
        "語彙の数、および語彙の最初の10語を表示します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo94L-N5tmxf"
      },
      "source": [
        "print(len(model.wv.index2word))  # 語彙の数\n",
        "print(model.wv.index2word[:10])  # 最初の10単語"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUCZ4e07tMo5"
      },
      "source": [
        "## 単語同士の演算\n",
        "`most_similar`メソッドにより、ある単語に類似度の高い単語を、類似度が高い順に表示することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC1PdSty2B8h"
      },
      "source": [
        "model.wv.most_similar(\"猫\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRZ1Lqoe-9cT"
      },
      "source": [
        "単語の類似度は以下の式で表されるコサイン類似度で計算しています。  \n",
        "ベクトル$\\vec{a}=(a_1,a_2,\\cdots, a_n)$、$\\vec{b}=(b_1,b_2,\\cdots, b_n)$として、\n",
        "$$\\frac{a_1b_1+a_2b_2+\\cdots + a_nb_n}{\\sqrt{a_1^2+a_2^2+\\cdots+a_n^2}\\sqrt{b_1^2+b_2^2+\\cdots+b_n^2}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDrWPGc4-tdZ"
      },
      "source": [
        "word2vecにより、単語同士の足し算、引き算が可能になります。  \n",
        "以下は単語同士の演算の例ですが、単語のベクトル同士で演算が行われ、結果に最も近い単語が表示されます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1GLiuZItMo5"
      },
      "source": [
        "model.wv.most_similar(positive=[\"猫\", \"人間\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OccbmzGOtMo7"
      },
      "source": [
        "positiveは足し合わせる単語のリストです。  \n",
        "従って、上記は\n",
        "**猫 + 人間**\n",
        "を計算しています。\n",
        "\n",
        "以下では、  \n",
        "**人間 + 猫 - 夢**  \n",
        "を演算しています。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05ytAJuKtMo8"
      },
      "source": [
        "model.wv.most_similar(positive=[\"人間\", \"猫\"], negative=[\"夢\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y_oQ2gftMo-"
      },
      "source": [
        "negativeは引く単語のリストです。"
      ]
    }
  ]
}